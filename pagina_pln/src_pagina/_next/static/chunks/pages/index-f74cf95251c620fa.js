(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[405],{5557:function(e,a,s){(window.__NEXT_P=window.__NEXT_P||[]).push(["/",function(){return s(5901)}])},2677:function(e,a,s){"use strict";s.d(a,{Z:function(){return c}});var n=s(5893),o=s(9008),i=s.n(o),t=s(1163),r=s(1664),l=s.n(r);function c(e){let{title:a,desc:s,children:o}=e,{asPath:r}=(0,t.useRouter)();return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(i(),{children:[(0,n.jsx)("title",{children:"Aplicaci\xf3n de PLN"}),(0,n.jsx)("meta",{name:"description",content:"Generated by create next app"}),(0,n.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1"}),(0,n.jsx)("link",{rel:"icon",href:"/favicon.ico"})]}),(0,n.jsx)("main",{className:"container mx-auto",children:(0,n.jsx)("div",{className:"font-sans bg-white flex flex-col min-h-screen w-full",children:(0,n.jsxs)("div",{children:[(0,n.jsx)("div",{className:"bg-gray-200 px-4 py-8",children:(0,n.jsxs)("div",{className:"w-full md:max-w-6xl md:mx-auto md:flex md:items-center md:justify-between",children:[(0,n.jsx)("div",{children:(0,n.jsx)(l(),{href:"#",className:"inline-block py-2 text-gray-800 text-2xl font-bold",children:"Procesamiento de Lenguaje Natural"})}),(0,n.jsx)("div",{children:(0,n.jsxs)("div",{className:"hidden md:block",children:[(0,n.jsx)(l(),{href:"/",className:"inline-block py-1 md:py-4 text-gray-600 mr-6 "+("/"===r?"font-bold":""),children:"Introducci\xf3n"}),(0,n.jsx)(l(),{href:"/analysis",className:"inline-block py-1 md:py-4 text-gray-600 hover:text-gray-600 mr-6 "+("/analysis"===r?"font-bold":""),children:"An\xe1lisis"}),(0,n.jsx)(l(),{href:"/results",className:"inline-block py-1 md:py-4 text-gray-600 hover:text-gray-600 mr-6 "+("/results"===r?"font-bold":""),children:"Resultados"})]})})]})}),(0,n.jsxs)("div",{className:"bg-gray-200 md:overflow-hidden",children:[(0,n.jsx)("div",{className:"px-4",children:(0,n.jsxs)("div",{className:"relative w-full md:max-w-2xl md:mx-auto text-center",children:[(0,n.jsx)("h1",{className:"font-bold text-gray-700 text-xl sm:text-2xl md:text-5xl leading-tight mb-10",children:a}),(0,n.jsx)("p",{className:"text-gray-600 md:text-xl md:px-18",children:s}),(0,n.jsx)("div",{className:"hidden md:block h-32 w-32 rounded-full bg-blue-800 absolute right-0 bottom-0 -mb-32 -mr-48"}),(0,n.jsx)("div",{className:"hidden md:block h-12 w-12 rounded-full bg-yellow-500 absolute top-0 right-0 -mr-64 mt-14"})]})}),(0,n.jsx)("svg",{className:"fill-current bg-gray-200 text-white hidden md:block",xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 1440 220",children:(0,n.jsx)("path",{fillOpacity:"1",d:"M0,64L120,85.3C240,107,480,149,720,149.3C960,149,1200,107,1320,85.3L1440,64L1440,320L1320,320C1200,320,960,320,720,320C480,320,240,320,120,320L0,320Z"})})]}),(0,n.jsx)("div",{className:"container mx-auto px-4 -mt-18",children:o}),(0,n.jsxs)("p",{className:"text-center p-4 text-gray-600 mt-10",children:["Proyecto PLN:",(0,n.jsx)(l(),{className:"border-b text-blue-500",href:"https://twitter.com/mithicher",target:"_blank",children:" Universidad de Sonora"}),(0,n.jsx)("span",{children:" - 2023"})]}),(0,n.jsx)("div",{className:"flex mx-auto px-4 -mt-18 mb-7",style:{justifyContent:"center"},children:(0,n.jsx)("img",{src:"escudo.png",style:{height:120}})})]})})})]})}},5901:function(e,a,s){"use strict";s.r(a),s.d(a,{default:function(){return P}});var n=s(5893),o=s(2677),i=s(9641);let{tituloHeader:t,descripcionHeader:r,seccionUnoTitulo:l,seccionUnoParteUno:c,seccionUnoImagenUno:d,seccionUnoParteDos:m,seccionUnoImagenDos:u,seccionDosTitulo:x,seccionDosParteUno:p,seccionDosImagenUno:f,seccionDosParteDos:b,seccionDosImagenDos:g,seccionTresTitulo:h,seccionTresParteUno:v,seccionTresImagenUno:y,seccionTresParteDos:j,seccionTresImagenDos:N}=i.f3;function P(){return(0,n.jsxs)(o.Z,{title:t,desc:r,children:[(0,n.jsx)("h2",{className:"font-bold text-gray-600 text-xl sm:text-2xl md:text-4xl leading-tight mb-6",children:l}),(0,n.jsxs)("div",{className:"grid grid-cols-2 gap-4 mb-12",style:{alignItems:"center",justifyItems:"center"},children:[(0,n.jsx)("div",{children:c}),(0,n.jsx)("img",{src:d,style:{maxHeight:400},alt:"seccion-uno-imagen-uno"})]}),(0,n.jsxs)("div",{className:"grid grid-cols-2 gap-4 mb-12",style:{alignItems:"center",justifyItems:"center"},children:[(0,n.jsx)("img",{src:u,style:{maxHeight:320},alt:"seccion-uno-imagen-dos"}),(0,n.jsx)("div",{children:m})]}),(0,n.jsx)("h2",{className:"font-bold text-gray-600 text-xl sm:text-2xl md:text-4xl leading-tight mb-6",children:x}),(0,n.jsxs)("div",{className:"grid grid-cols-2 gap-4 mb-12",style:{alignItems:"center",justifyItems:"center"},children:[(0,n.jsx)("div",{children:p}),(0,n.jsx)("img",{src:f,style:{maxHeight:120}})]}),(0,n.jsxs)("div",{className:"grid grid-cols-2 gap-4 mb-12",style:{alignItems:"center",justifyItems:"center"},children:[(0,n.jsx)("div",{children:b}),(0,n.jsx)("img",{src:g,style:{maxHeight:420}})]}),(0,n.jsx)("h2",{className:"font-bold text-gray-600 text-xl sm:text-2xl md:text-4xl leading-tight mb-6",children:h}),(0,n.jsxs)("div",{className:"grid grid-cols-2 gap-4 mb-12",style:{alignItems:"center",justifyItems:"center"},children:[(0,n.jsx)("div",{children:v}),(0,n.jsx)("img",{src:y,style:{maxHeight:220}})]}),(0,n.jsxs)("div",{className:"grid grid-cols-2 gap-4 mb-12",style:{alignItems:"center",justifyItems:"center"},children:[(0,n.jsx)("img",{src:N,style:{maxHeight:220}}),(0,n.jsx)("div",{children:j})]})]})}},9641:function(e){"use strict";e.exports=JSON.parse('{"f3":{"tituloHeader":"Introducci\xf3n","descripcionHeader":"El objetivo es elaborar un tablero que muestre una gr\xe1fica en linea de tiempo, con cortes semanales, de los principales temas de inter\xe9s nacional e internacional, desde el punto de vista oficialista de la presidencia de Andr\xe9s Manuel L\xf3pez Obrador.","seccionUnoTitulo":"Planteamiento del problema","seccionUnoParteUno":"Para realizar esta tarea usaremos las ma\xf1aneras que ha estado haciendo el presidente, las cuales las tomaremos del canal de Youtube oficial del presidente. Despu\xe9s, preprocesaremos los datos para optimizar tanto el tiempo como el resultado del modelo que usaremos para extraer los principales temas (topic modeling).","seccionUnoImagenUno":"seccion_1_1.png","seccionUnoParteDos":"Este modelo sera Asignaci\xf3n Latente de Dirichlet (LDA por sus siglas en ingles), en el cual cada documento se considera una mezcla de varios temas latentes, y cada tema se define como una distribuci\xf3n de probabilidad sobre un conjunto de palabras. Para finalizar, analizaremos los resultados e identificaremos los temas principales hablamos en las ma\xf1aneras en cada semana.","seccionUnoImagenDos":"seccion_1_2.jpg","seccionDosTitulo":"Obtenci\xf3n de datos","seccionDosParteUno":"El inicio de este proyecto radica en la adquisici\xf3n de datos. En un principio, pensamos en utilizar las transcripciones de las conferencias matutinas, disponibles en un repositorio de GitHub llamado “conferencias matutinas AMLO”. Este repositorio alberga todas las transcripciones de las conferencias desde la primera hasta la m\xe1s reciente. No obstante, nos enfrentamos a un desaf\xedo significativo: al depurar estos datos, nos encontrar\xedamos con un exceso de contenido irrelevante proveniente de las conversaciones del presidente con los periodistas entrevistadores.","seccionDosImagenUno":"seccion_2_1.png","seccionDosParteDos":"Despues exploramos otras opciones y descubrimos que los videos de las conferencias matutinas publicados en YouTube incluyen una descripci\xf3n donde condensan la informaci\xf3n tratada en el respectivo d\xeda. Optamos por esta alternativa, ya que toda la informaci\xf3n relevante estaba ya sintetizada en dichas descripciones, eliminando la necesidad de lidiar con datos irrelevantes.","seccionDosImagenDos":"seccion_2_2.png","seccionTresTitulo":"Llamada a API de YouTube","seccionTresParteUno":"El siguiente paso en nuestro proceso es la extracci\xf3n de las descripciones de cada conferencia matutina. Dada la cantidad existente de alrededor de 1,100 vıdeos, la extracci\xf3n manual no es una opci\xf3n viable. Tras una investigaci\xf3n preliminar, descubrimos la API de YouTube, una herramienta gratuita que resulta ser de gran utilidad para nuestra tarea.","seccionTresImagenUno":"seccion_3_1.jpg","seccionTresParteDos":"Todas las conferencias matutinas se encuentran agrupadas en una lista de reproducci\xf3n creada por el canal oficial del presidente Andr\xe9s Manuel L\xf3pez Obrador. Hemos determinado que, mediante una solicitud a la API de YouTube, podemos recopilar toda la informaci\xf3n (t\xedtulo, fecha de publicaci\xf3n, descripci\xf3n, etc.) de cada video contenido en esta lista de reproducci\xf3n.","seccionTresImagenDos":"seccion_3_2.png"},"YF":{"tituloHeader":"An\xe1lisis de datos","descripcionHeader":"Dividida en dos partes, preprocesamiento, la cual se encarga de limpiar y formatear la informaci\xf3n de la manera solicitada, y procesamiento principal que es donde se desarrollan los valores y se aplican los algoritmos.","seccionUnoTitulo":"Preprocesamiento: Stop words, lematizaci\xf3n y normalizaci\xf3n","seccionUnoParteUno":"Empezaremos con la eliminaci\xf3n de las stopwords. Las stopwords son palabras que no nos dan informaci\xf3n sobre el contenido de un texto, como conjunciones, art\xedculos, preposiciones y adverbios. Por lo tanto, como nuestro objetivo es solo obtener los temas que el presidente hablo, estas palabras no nos son \xfatiles. La lista de stopwords fue obtenida de la paqueter\xeda nltk. La lematizaci\xf3n consiste en reducir las palabras a su forma base o ”lemas”, es decir, a los verbos los reduce a su forma en infinitivo y los sustantivos a su forma singular masculina.","seccionUnoParteUno2":"En otras palabras, la lematizaci\xf3n es el proceso de encontrar la forma base de una palabra, a partir de su variante flexionada. Como normalizaci\xf3n eliminaremos signos de puntaci\xf3n y las may\xfasculas, solo dejaremos la n con virgulilla, ya que es una letra mas en el abecedario en espa\xf1ol.","seccionUnoParteDos":"Por ultimo, vectorizamos los documentos usando la vectorizaci\xf3n frecuencia de t\xe9rmino – frecuencia inversa de documento, TF-idf por sus siglas en ingles. Con esto, podemos representar los documentos como vectores en un espacio con dimensionalidad alta. Donde cada palabra del documento es considerada como una caracter\xedstica separada o dimensi\xf3n, y su valor en cada dimensi\xf3n representa la importancia de cada palabra en el documento. Esto es, al realizar la vectorizaci\xf3n obtenemos una matriz nxm, donde n es el numero de documentos y m el tama\xf1o del vocabulario. Entonces, si tomamos la i-\xe9sima fila y la j-\xe9sima columna estar\xedamos obteniendo la importancia que tiene la palabra j en el documento i.","seccionUnoParteTres":"Para tokenizar las palabras usamos la tokenizaci\xf3n Penn Treebank. Incluimos 1-gramas, que son las palabras solas, y 2-gramas, que serian tokens con dos palabras para no perder la informaci\xf3n de las palabras compuestas. Tambi\xe9n incluimos dos par\xe1metros. El primero elimina las palabras que aparecen mas del 70 % de los documento, estas palabras se repiten tanto que no podremos identificarlas solo en uno de los temas. El segundo elimina las palabras que a lo mas aparecen en 2 documentos, al aparecer tan poco son palabras que no presentan una relaci\xf3n estrecha con los temas.","seccionDosTitulo":"Procesamiento: MaxAbs y Keywords","seccionDosParteUno":"Creamos instancia de la clase Truncated SVD, despues justamos la transfromacion con nuestros datos utilizando la funcion fit(vector escalado), despues reducimos la dimensionalidad con el m\xe9todo de transform(vector escalado).","seccionDosParteDos":"Como podemos ver del an\xe1lisis de componentes principales, no podemos sacar ninguna conclusi\xf3n sobre la cantidad de t\xf3picos del corpus.","seccionDosParteTres":""},"K3":{"tituloHeader":"Resultados obtenidos","descripcionHeader":"En esta secci\xf3n se comparten los datos obtenidos y las observaciones que logramos captar a trav\xe9s del estudio de los diferentes textos.","seccionUnoTitulo":"Visualizaci\xf3n con herramienta","seccionUnoParteUno":"Continuamos con los siguientes pasos en el procesamiento principal: Obtener el vocabulario de t\xe9rminos, obtener las frecuencias de t\xe9rminos en el primer documento, crear una lista de tuplas que contienen el t\xe9rmino y su frecuencia en el primer documento, ordenar la lista de tuplas por la frecuencia descendente, obtener las 10 palabras clave m\xe1s importantes para el primer documento. Para la ultima visualizacion en el notebook, se siguieron los siguientes pasos: Tokenizar los documentos utilizando el tokenizador de Penn Treebank, Crear un diccionario de t\xe9rminos a partir de los documentos, Crear una representaci\xf3n de la bolsa de palabras de los documentos, Entrenar un modelo LDA con 10 t\xf3picos, Imprimir y Crear una visualizaci\xf3n con pyLDAvis los t\xf3picos.","seccionDosTitulo":"Datos adicionales","seccionDosParteUno":"Con el fin de analizar la diferencia entre las keywords obtenidas a traves de las semanas utilizando nuestro proceso, pudimos formar una gr\xe1fica para ilustrar las diferencias de topicos por semana. La cual nos puede ayudar a visualizar la manera en que un topico es que tan similar o que tan distinto a los otros topicos.","seccionDosParteDos":"Con estos datos se concluyen las observaciones."}}')}},function(e){e.O(0,[814,774,888,179],function(){return e(e.s=5557)}),_N_E=e.O()}]);