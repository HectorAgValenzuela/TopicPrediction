<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Aplicación de PLN</title><meta name="description" content="Generated by create next app"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="5"/><link rel="preload" href="./_next/static/css/8f7e17c9889b628f.css" as="style"/><link rel="stylesheet" href="./_next/static/css/8f7e17c9889b628f.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="./_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="./_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="./_next/static/chunks/main-4dcb7f9b52833aba.js" defer=""></script><script src="./_next/static/chunks/pages/_app-7172e87d084d5d88.js" defer=""></script><script src="./_next/static/chunks/ee8b1517-c26901c3de25710d.js" defer=""></script><script src="./_next/static/chunks/814-3cf97f3d7f95146a.js" defer=""></script><script src="./_next/static/chunks/352-c6a818aa65806843.js" defer=""></script><script src="./_next/static/chunks/796-28472cbaad4ded9d.js" defer=""></script><script src="./_next/static/chunks/pages/analysis-1a74070bf87b234e.js" defer=""></script><script src="./_next/static/_4bjyjnUnsleDwjF8ND5p/_buildManifest.js" defer=""></script><script src="./_next/static/_4bjyjnUnsleDwjF8ND5p/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="container mx-auto"><div class="font-sans bg-white flex flex-col min-h-screen w-full"><div><div class="bg-gray-200 px-4 py-8"><div class="w-full md:max-w-6xl md:mx-auto md:flex md:items-center md:justify-between"><div><a class="inline-block py-2 text-gray-800 text-2xl font-bold" href="/analysis#">Procesamiento de Lenguaje Natural</a></div><div><div class="hidden md:block"><a class="inline-block py-1 md:py-4 text-gray-600 mr-6 " href="/">Introducción</a><a class="inline-block py-1 md:py-4 text-gray-600 hover:text-gray-600 mr-6 font-bold" href="/analysis">Análisis</a><a class="inline-block py-1 md:py-4 text-gray-600 hover:text-gray-600 mr-6 " href="/results">Resultados</a></div></div></div></div><div class="bg-gray-200 md:overflow-hidden"><div class="px-4"><div class="relative w-full md:max-w-2xl md:mx-auto text-center"><h1 class="font-bold text-gray-700 text-xl sm:text-2xl md:text-5xl leading-tight mb-10">Análisis de datos</h1><p class="text-gray-600 md:text-xl md:px-18">Dividida en dos partes, preprocesamiento, la cual se encarga de limpiar y formatear la información de la manera solicitada, y procesamiento principal que es donde se desarrollan los valores y se aplican los algoritmos.</p><div class="hidden md:block h-32 w-32 rounded-full bg-blue-800 absolute right-0 bottom-0 -mb-32 -mr-48"></div><div class="hidden md:block h-12 w-12 rounded-full bg-yellow-500 absolute top-0 right-0 -mr-64 mt-14"></div></div></div><svg class="fill-current bg-gray-200 text-white hidden md:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 220"><path fill-opacity="1" d="M0,64L120,85.3C240,107,480,149,720,149.3C960,149,1200,107,1320,85.3L1440,64L1440,320L1320,320C1200,320,960,320,720,320C480,320,240,320,120,320L0,320Z"></path></svg></div><div class="container mx-auto px-4 -mt-18"><h2 class="font-bold text-gray-600 text-xl sm:text-2xl md:text-4xl leading-tight mb-6">Flujo preprocesamiento</h2><div class="grid grid-cols-1 gap-4 mb-12">Los pasos del preprocesamiento son los siguientes:<br/>1. Eliminación de stopwords<br/>2. lexematización del texto<br/>3. Eliminación de stopwords<br/>4. Eliminación de verbos<br/>5. Normalización del texto<br/>6. Vectorización con n-gramas<br/></div><div class="grid grid-cols-1 gap-4 mb-12" style="align-items:center;justify-items:center"><div style="display:flex;justify-self:center"><img src="pln.png" style="max-height:580px" alt="seccion-uno-imagen-dos"/></div><div></div></div><h2 class="font-bold text-gray-600 text-xl sm:text-2xl md:text-4xl leading-tight mb-6">Preprocesamiento: Stop words, lematización y normalización</h2><div class="grid grid-cols-2 gap-4 mb-8" style="align-items:center;justify-items:center"><div>Empezaremos con la eliminación de las stopwords. Las stopwords son palabras que no nos dan información sobre el contenido de un texto, como conjunciones, artículos, preposiciones y adverbios. Por lo tanto, como nuestro objetivo es solo obtener los temas que el presidente hablo, estas palabras no nos son útiles. La lista de stopwords fue obtenida de la paquetería nltk.<br/><br/>La lematización consiste en reducir las palabras a su forma base o ”lemas”, es decir, a los verbos los reduce a su forma en infinitivo y los sustantivos a su forma singular masculina.<br/><br/>En otras palabras, la lematización es el proceso de encontrar la forma base de una palabra, a partir de su variante flexionada. Como normalización eliminaremos signos de puntación y las mayúsculas, solo dejaremos la n con virgulilla, ya que es una letra mas en el abecedario en español.<br/><br/>También eliminamos los verbos, ya que la mayoría de las palabras que nos dan información sobre el tema que se está tratando son sustantivos, por lo que se pueden obtener mucho mejores resultados si nos olvidamos de los verbos.</div><div class="container mx-auto"><canvas role="img" height="200" width="300"></canvas></div></div><h2 class="font-bold text-gray-600 text-xl sm:text-2xl md:text-4xl leading-tight mb-6">Preprocesamiento: Vectorización</h2><div class="grid grid-cols-1 gap-4 mb-12" style="align-items:center;justify-items:center">Por ultimo, vectorizamos los documentos usando la vectorización frecuencia de término – frecuencia inversa de documento, TF-idf por sus siglas en ingles. Con esto, podemos representar los documentos como vectores en un espacio con dimensionalidad alta. Donde cada palabra del documento es considerada como una característica separada o dimensión, y su valor en cada dimensión representa la importancia de cada palabra en el documento. Esto es, al realizar la vectorización obtenemos una matriz nxm, donde n es el numero de documentos y m el tamaño del vocabulario. Entonces, si tomamos la i-ésima fila y la j-ésima columna estaríamos obteniendo la importancia que tiene la palabra j en el documento i.<div style="display:flex;justify-self:center"><img src="variancia.png" style="max-height:320px" alt="vect_1"/><img src="variancia2.png" style="max-height:320px" alt="vect_2"/><img src="variancia3.png" style="max-height:320px" alt="vect_3"/></div>Para tokenizar las palabras usamos la tokenización Penn Treebank. Incluimos 1-gramas, que son las palabras solas, y 2-gramas, que serian tokens con dos palabras para no perder la información de las palabras compuestas. También incluimos dos parámetros. El primero elimina las palabras que aparecen mas del 70 % de los documento, estas palabras se repiten tanto que no podremos identificarlas solo en uno de los temas. El segundo elimina las palabras que a lo mas aparecen en 2 documentos, al aparecer tan poco son palabras que no presentan una relación estrecha con los temas.</div><h2 class="font-bold text-gray-600 text-xl sm:text-2xl md:text-4xl leading-tight mb-6">Procesamiento: MaxAbs y Keywords</h2><div class="grid grid-cols-2 gap-4 mb-8" style="align-items:center;justify-items:center"><div class="container mx-auto"><canvas role="img" height="150" width="300"></canvas></div><div>Con el análisis TF-IDF buscamos las 10 palabras clave m ́as importantes de cada documento y los agregamos como una columna en el dataframe.<br/>Creamos instancia de la clase Truncated SVD, despues justamos la transfromacion con nuestros datos utilizando la funcion fit(vector escalado), despues reducimos la dimensionalidad con el método de transform(vector escalado).</div><div style="text-align:center">Como podemos ver del análisis de componentes principales, no podemos sacar ninguna conclusión sobre la cantidad de tópicos del corpus.</div></div></div><p class="text-center p-4 text-gray-600 mt-10">Proyecto PLN:<a class="border-b text-blue-500" target="_blank" href="https://twitter.com/mithicher"> Universidad de Sonora</a><span> - 2023</span></p><div class="flex mx-auto px-4 -mt-18 mb-7" style="justify-content:center"><img src="escudo.png" style="height:120px"/></div></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/analysis","query":{},"buildId":"_4bjyjnUnsleDwjF8ND5p","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>